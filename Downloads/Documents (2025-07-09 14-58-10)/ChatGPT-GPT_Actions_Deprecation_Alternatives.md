---
title: GPT Actions Deprecation Alternatives
source: https://chatgpt.com/c/686c1138-391c-8003-b792-977cb18de248
---

# GPT Actions Deprecation Alternatives

#### You:
<time datetime="2025-07-07T18:25:59.421Z" title="7/7/2025, 1:25:59 PM">13:25</time>

https://openai.com/index/computer-using-agent/ Browser base custom gpt action schema

#### ChatGPT:
<time datetime="2025-07-07T18:26:01.092Z" title="7/7/2025, 1:26:01 PM">13:26</time>

îˆƒOpenAI's GPT Actions, which enabled Custom GPTs to interact with external APIs via OpenAPI schemas, were deprecated in early 2024.îˆ„ îˆƒThis feature allowed developers to define API endpoints, parameters, and authentication methods, enabling GPTs to perform tasks like data retrieval and workflow automation.îˆ„ îˆƒHowever, due to complexities in setup and maintenance, OpenAI has shifted focus to more robust solutions like function calling and the Assistants APIîˆ„ îˆ€citeîˆ‚turn0search4îˆ.îˆ†

### Understanding GPT Actions

îˆƒGPT Actions allowed Custom GPTs to make API calls by importing OpenAPI schemas (versions 3.0 or 3.1).îˆ„ îˆƒDevelopers could specify endpoints, request/response structures, and authentication details, enabling GPTs to perform actions such as fetching data or triggering workflowsîˆ„ îˆ€citeîˆ‚turn0search5îˆ.îˆ†

### Alternatives Post-Deprecation

With the deprecation of GPT Actions, developers have several alternatives:

- **Function Calling**: îˆƒOpenAI's function calling feature allows GPTs to call functions defined by developers, facilitating structured outputs and integrations.îˆ„îˆ†

- **Assistants API**: îˆƒThis API provides tools for building AI assistants with persistent memory and the ability to call functions, offering a more comprehensive solution for complex tasks.îˆ„îˆ†

- **Third-Party Platforms**: îˆƒPlatforms like Lindy offer AI agents capable of multi-step workflows, memory retention, and integrations with tools like Gmail, Slack, and Salesforceîˆ„ îˆ€citeîˆ‚turn0search4îˆ.îˆ†

### Resources for Building Custom Integrations

For developers interested in building custom integrations:

- **OpenAI Cookbook**: îˆƒProvides examples and guides for integrating GPTs with various APIs and servicesîˆ„ îˆ€citeîˆ‚turn0search2îˆ.îˆ†

- **Community Repositories**: îˆƒGitHub repositories like [bapo2/gpt-actions](https://github.com/bapo2/gpt-actions) offer a collection of action schemas that can serve as references.îˆ„îˆ†

If you need assistance transitioning from GPT Actions to these alternatives or have specific integration requirements, feel free to ask.


#### You:
<time datetime="2025-07-07T18:26:26.467Z" title="7/7/2025, 1:26:26 PM">13:26</time>

Now apply to my case use great thanks 

#### ChatGPT:
<time datetime="2025-07-07T18:26:28.176Z" title="7/7/2025, 1:26:28 PM">13:26</time>

Alright Helen, let's cut through the noise and get you back on track.

### ğŸ§¨ The Brutal Truth

îˆƒ\*\*GPT Actions are dead.\*\*îˆ„ îˆƒOpenAI pulled the plug in early 2024.îˆ„ îˆƒNo more browser-based OpenAPI schema uploads.îˆ„ îˆƒNo more plug-and-play integrations.îˆ„ îˆƒNo more magic.îˆ„ îˆƒIf you're still trying to revive that corpse, you're wasting your time.îˆ„îˆ†

### ğŸ§  The New Reality: Function Calling via Assistants API

îˆƒOpenAI has shifted gears.îˆ„ îˆƒNow, it's all about **Function Calling** through the **Assistants API**.îˆ„ îˆƒThis means you define your functions, and the model decides when to call them.îˆ„ îˆƒIt's more flexible but requires more setup.îˆ„îˆ†

### ğŸ› ï¸ Your Action Plan

1. **Define Your Functions**: îˆƒCreate JSON schemas for the functions you want the model to call.îˆ„îˆ†

2. **Set Up the Assistants API**: îˆƒUse the Assistants API to register your functions and manage conversations.îˆ„îˆ†

3. **Handle Function Calls**: îˆƒWhen the model decides to call a function, it will return the function name and arguments.îˆ„ îˆƒYour code needs to execute the function and return the result to the model.îˆ„îˆ†

4. **Manage Conversations**: îˆƒUse threads to maintain conversation history and context.îˆ„îˆ†

### ğŸ§ª Example: Weather Assistant

îˆƒHere's a simplified example of a weather assistant using the Assistants API:îˆ„îˆ†

```python
import openai
import requests

openai.api_key = 'your-api-key'

# Define the function schema
functions = [
    {
        "name": "get_weather",
        "description": "Get the current weather for a location.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and country, e.g., Paris, France"
                }
            },
            "required": ["location"]
        }
    }
]

# Define the function implementation
def get_weather(location):
    # Replace with your actual weather API call
    return f"The weather in {location} is sunny."

# Create an assistant
assistant = openai.beta.assistants.create(
    instructions="You are a weather assistant.",
    model="gpt-4",
    tools=[{"type": "function", "function": functions[0]}]
)

# Create a thread
thread = openai.beta.threads.create()

# Add a message to the thread
openai.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="What's the weather in Paris?"
)

# Run the assistant
run = openai.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id
)

# Poll for the run status
while True:
    run_status = openai.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )
    if run_status.status == "completed":
        break
    elif run_status.status == "requires_action":
        tool_calls = run_status.required_action.submit_tool_outputs.tool_calls
        for call in tool_calls:
            if call.function.name == "get_weather":
                location = call.function.arguments["location"]
                weather = get_weather(location)
                openai.beta.threads.runs.submit_tool_outputs(
                    thread_id=thread.id,
                    run_id=run.id,
                    tool_outputs=[{
                        "tool_call_id": call.id,
                        "output": weather
                    }]
                )
    else:
        time.sleep(1)

# Retrieve the assistant's response
messages = openai.beta.threads.messages.list(thread_id=thread.id)
for message in messages.data:
    print(f"{message.role}: {message.content}")
```

### ğŸ§± Resources

- [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)
- [Assistants API Overview](https://platform.openai.com/docs/assistants/overview)
- [Function Calling with OpenAPI Spec](https://cookbook.openai.com/examples/function_calling_with_an_openapi_spec)

### ğŸš€ Next Steps

îˆƒIf you're ready to move forward, start by defining your function schemas and setting up the Assistants API.îˆ„ îˆƒIf you need help with specific parts of the process, let me know, and we can tackle them together.îˆ„îˆ†

îˆƒLet's get this done.îˆ„îˆ†


#### You:
<time datetime="2025-07-07T18:54:45.007Z" title="7/7/2025, 1:54:45 PM">13:54</time>

let's do it 

#### ChatGPT:
<time datetime="2025-07-07T18:54:45.911Z" title="7/7/2025, 1:54:45 PM">13:54</time>

Fantastic, Helen. Let's roll up our sleeves and get this Function Calling setup operational. Here's the streamlined plan:

---

### ğŸ› ï¸ Step 1: Define Your Function

îˆƒWe'll start by defining the function that the assistant can call.îˆ„ îˆƒFor example, a `get_weather` function that retrieves weather information for a given location.îˆ„îˆ†

```python
functions = [
    {
        "name": "get_weather",
        "description": "Retrieve current weather information for a specified location.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City and country, e.g., Paris, France"
                }
            },
            "required": ["location"]
        }
    }
]
```

---

### ğŸ”§ Step 2: Implement the Function

îˆƒNext, implement the actual function logic.îˆ„ îˆƒThis function will be called when the assistant determines it's appropriate.îˆ„îˆ†

```python
import requests

def get_weather(location):
    # Replace with your actual weather API endpoint and API key
    api_key = "your_api_key"
    base_url = "https://api.openweathermap.org/data/2.5/weather"
    params = {"q": location, "appid": api_key, "units": "metric"}
    response = requests.get(base_url, params=params)
    data = response.json()
    weather = data["weather"][0]["description"]
    temperature = data["main"]["temp"]
    return f"The weather in {location} is {weather} with a temperature of {temperature}Â°C."
```

---

### ğŸ§  Step 3: Create the Assistant

îˆƒUsing the OpenAI Assistants API, create an assistant and register the function.îˆ„îˆ†

```python
import openai

openai.api_key = "your_openai_api_key"

assistant = openai.beta.assistants.create(
    instructions="You are a helpful assistant that provides weather information.",
    model="gpt-4",
    tools=[{"type": "function", "function": functions[0]}]
)
```

---

### ğŸ§µ Step 4: Start a Conversation Thread

îˆƒCreate a thread to manage the conversation context.îˆ„îˆ†

```python
thread = openai.beta.threads.create()
```

---

### ğŸ’¬ Step 5: Add a User Message

îˆƒAdd a message from the user to the thread.îˆ„îˆ†

```python
openai.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="What's the weather in Paris?"
)
```

---

### ğŸš€ Step 6: Run the Assistant

îˆƒExecute the assistant to process the message and determine if a function call is needed.îˆ„îˆ†

```python
run = openai.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id
)
```

---

### ğŸ”„ Step 7: Handle Function Calls

îˆƒIf the assistant decides to call a function, handle the function call and provide the output back to the assistant.îˆ„îˆ†

```python
import time

while True:
    run_status = openai.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )
    if run_status.status == "completed":
        break
    elif run_status.status == "requires_action":
        tool_calls = run_status.required_action.submit_tool_outputs.tool_calls
        for call in tool_calls:
            if call.function.name == "get_weather":
                location = call.function.arguments["location"]
                weather_info = get_weather(location)
                openai.beta.threads.runs.submit_tool_outputs(
                    thread_id=thread.id,
                    run_id=run.id,
                    tool_outputs=[{
                        "tool_call_id": call.id,
                        "output": weather_info
                    }]
                )
    else:
        time.sleep(1)
```

---

### ğŸ“© Step 8: Retrieve the Assistant's Response

îˆƒFinally, retrieve the assistant's response after processing the function output.îˆ„îˆ†

```python
messages = openai.beta.threads.messages.list(thread_id=thread.id)
for message in messages.data:
    print(f"{message.role}: {message.content}")
```

---

This setup allows the assistant to process user inputs, determine when to call external functions, execute them, and provide the results back to the user seamlessly.

If you need further assistance or have specific requirements, feel free to ask!


#### You:
<time datetime="2025-07-07T18:56:42.001Z" title="7/7/2025, 1:56:42 PM">13:56</time>



#### Plugin (file_search):
<time datetime="2025-07-07T18:56:54.482Z" title="7/7/2025, 1:56:54 PM">13:56</time>

Make sure to include `ã€message_idxâ€ sourceã€‘` markers to provide citations based on this file, where [message_idx] is provided at the beginning of this message
{
  "summon-lore": {
    "function": "summon_lore",
    "parameters": {
      "archetype": "string"
    }
  },
  "trigger-banishment": {
    "function": "trigger_banishment",
    "parameters": {
      "entity_id": "string",
      "dimension": "string"
    }
  }
}

#### ChatGPT:
<time datetime="2025-07-07T18:56:42.722Z" title="7/7/2025, 1:56:42 PM">13:56</time>

Perfect. You dropped in `triggerMap.json`, and I see youâ€™ve got a classic function mapping setup designed for an agent or assistant interface to call `summon_lore` and `trigger_banishment`. Letâ€™s weaponize this into a fully operational **Function Calling Assistant** using the OpenAI Assistants API.

---

## ğŸ§© Step-by-Step: Translate `triggerMap.json` into Assistant Function Calls

### âœ… 1. **Parse Your Schema**

Youâ€™ve defined two callable functions:

#### a. `summon_lore`

```json
{
  "function": "summon_lore",
  "parameters": {
    "archetype": "string"
  }
}
```

#### b. `trigger_banishment`

```json
{
  "function": "trigger_banishment",
  "parameters": {
    "entity_id": "string",
    "dimension": "string"
  }
}
```

---

### ğŸ§  2. **Define These for the Assistants API**

Hereâ€™s how we translate them into Python for OpenAI:

```python
functions = [
    {
        "name": "summon_lore",
        "description": "Summons lore details for a specific archetype.",
        "parameters": {
            "type": "object",
            "properties": {
                "archetype": {
                    "type": "string",
                    "description": "The archetype to summon lore for, e.g., 'Trickster', 'Rebel', 'Oracle'"
                }
            },
            "required": ["archetype"]
        }
    },
    {
        "name": "trigger_banishment",
        "description": "Banishes an entity from the known world into a specified dimension.",
        "parameters": {
            "type": "object",
            "properties": {
                "entity_id": {
                    "type": "string",
                    "description": "Unique identifier for the entity to be banished"
                },
                "dimension": {
                    "type": "string",
                    "description": "Target dimension, e.g., 'Shadow Realm', 'Null Zone'"
                }
            },
            "required": ["entity_id", "dimension"]
        }
    }
]
```

---

### ğŸ›  3. **Implement Function Logic**

Stub logic for nowâ€”customize later:

```python
def summon_lore(archetype):
    return f"Lore for the {archetype}: A legendary figure with unmatched chaotic energy."

def trigger_banishment(entity_id, dimension):
    return f"Entity {entity_id} has been banished to {dimension}. Good riddance."
```

---

### ğŸ¤– 4. **Register Assistant**

```python
assistant = openai.beta.assistants.create(
    instructions="You are a chaotic lore-bending assistant that answers or acts upon fantasy requests.",
    model="gpt-4",
    tools=[{"type": "function", "function": f} for f in functions]
)
```

---

### ğŸ” 5. **Full Execution Loop**

Same execution as before â€” weâ€™ll poll the thread, intercept function calls, run them, and pass the output back.

If you want, Iâ€™ll drop this into a **runnable FastAPI or Flask** app, or script it out CLI-style. Just tell me:

- Do you want this headless (API only), or with a web interface?
- Want me to auto-generate examples like `summon_lore("Oracle")` and handle funky archetypes?

---

## â­ï¸ Next Move?

Pick a path:

1. **"Script It"** â€” Iâ€™ll drop a standalone file you can run to test locally.
2. **"Build the Flask/FastAPI server"** â€” Youâ€™ll get a real endpoint you can hit.
3. **"Go Full Stack"** â€” Want buttons + interface? Weâ€™ll drop it into React/TS too.
4. **"Deploy This"** â€” Iâ€™ll Dockerize and give you a deployment script.

You call it.
